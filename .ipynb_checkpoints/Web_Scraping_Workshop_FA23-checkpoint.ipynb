{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping with XPath and Python workshop\n",
    "We will be using XPath Helper in Google Chrome to select links from a webpage, and then use those links to download files from the webpage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will discuss XPath and XPath Helper. Go to the [XPath tutorial here](https://github.com/kaylaabner/WebScrapingWorkshop/blob/main/XPath_Tutorial.md).\n",
    "\n",
    "You need to add [XPath Helper](https://chrome.google.com/webstore/detail/xpath-helper/hgimnogjllphhhkhlmebbmlgjoejdpjl) to your Chromium-based browser (Google Chrome, Brave)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will be using a few libraries in Python to pull content from a website, using our knowledge of XPath to select exactly what we want on the page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # to interact with websites through Python\n",
    "from lxml import html # to use XPath in Python\n",
    "import pandas as pd # a data science package for handling structured data\n",
    "import random\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use the requests library. requests is a good entry to web scraping.\n",
    "# we will practice downloading a single file from the library's digital collections. \n",
    "\n",
    "r = requests.get('https://udspace.udel.edu/bitstream/handle/19716/5974/mss0109_0001-00.pdf') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r.text) #to retrieve the html of the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r.content) #to retrieve the content in bytes, used for downloading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use r.content to tell Python you want the file itself, and not the HTML from the page.\n",
    "#where this code says 'kabner', CHANGE IT to your NetID or name.\n",
    "\n",
    "os.mkdir('kabner') # create your own directory so we don't overwrite each others' files.\n",
    "\n",
    "with open('kabner/30406.pdf', 'wb') as f:\n",
    "    f.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's try to download a few PDFs from this collection.\n",
    "# let's make a list of a few URls we want to pull from. note that the URL is the exact location of the file itself. \n",
    "urls = ['https://udspace.udel.edu/bitstream/handle/19716/5974/mss0109_0001-00.pdf', 'https://udspace.udel.edu/bitstream/handle/19716/5975/mss0109_0002-00.pdf', 'https://udspace.udel.edu/bitstream/handle/19716/5976/mss0109_0003-00.pdf', 'https://udspace.udel.edu/bitstream/handle/19716/5977/mss0109_0004-00.pdf', 'https://udspace.udel.edu/bitstream/handle/19716/5978/mss0109_0005-00.pdf', 'https://udspace.udel.edu/bitstream/handle/19716/5979/mss0109_0006-00.pdf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now, we will create files named after the last 19 characters of the url so we can tell them apart.\n",
    "# CHANGE where it says 'kabner' to the name of your directory. \n",
    "\n",
    "for link in urls: # loop over list of URLs\n",
    "    r = requests.get(link) # tell requests to visit the site\n",
    "    print(str(link[-19:])) # print the filename (last 19 chars of the URL) so we know it's working\n",
    "    with open('kabner/' + str(link[-19:]), 'wb') as f: # open a file with our filenames (last 19 chars of URL)\n",
    "        f.write(r.content) # write the content of the page to the file, in this case, a PDF\n",
    "        time.sleep(5) # give the server a break. also keeps you from getting booted on certain sites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using XPath Helper to Select Links\n",
    "\n",
    "Go back to the [finding aid for the collection](https://library.udel.edu/special/findaids/view?docId=ead/mss0109.xml;tab=content). Let's use XPath Helper to figure out how to select the links on this page, so we can loop over them in Python, and download all the PDFs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will use our knowledge of XPath to select specific elements on the webpage.\n",
    "# I want to select a list of links so we can loop over them to download the PDFs. \n",
    " \n",
    "# Request the page\n",
    "page = requests.get('https://library.udel.edu/special/findaids/view?docId=ead/mss0109.xml;tab=content')\n",
    " \n",
    "# Parsing the page\n",
    "# (We need to use page.content rather than\n",
    "# page.text because html.fromstring implicitly\n",
    "# expects bytes as input.)\n",
    "tree = html.fromstring(page.content) # this is from our lxml package. \n",
    " \n",
    "# Get element using XPath\n",
    "links = tree.xpath(\"//a[@class='extlink']/@href\") \n",
    "type(links)\n",
    "\n",
    "working_links = links[:10] #we just want to select some of the links so as to not overwhelm the server.\n",
    "working_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to use the list of links to retrieve PDFs\n",
    "# remember to CHANGE 'kabner' to the name of your directory. \n",
    "\n",
    "for link in working_links:\n",
    "    r = requests.get(link)\n",
    "    print(str(link[-19:]))\n",
    "    with open('kabner/' + str(link[-19:]), 'wb') as f:\n",
    "        f.write(r.content)\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps: Reading in a List of URLs\n",
    "\n",
    "These instructions will allow you to create a text file of URLs using XPath Helper, and read that file in as a list so you can loop over it. This is a good option if you're having trouble parsing the HTML directly from the page. You can use XPath Helper to select all the links on a page, and just copy/paste them into a text file, then read them into Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to read in a text files of urls as a list so we can loop over it\n",
    "urls2 = open('path/to/urls.txt', 'r')\n",
    "\n",
    "urls3 = urls2.readlines()\n",
    "urls3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my text file has newlines at the end of each URL, so Python has trouble reading it.\n",
    "# remove newlines at the end of links from text file using this clean function. \n",
    "\n",
    "clean = [link.strip() for link in urls3]\n",
    "print(clean) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in clean:\n",
    "    r = requests.get(link)\n",
    "    print(str(link[-19:]))\n",
    "    with open(str(link[-19:]), 'wb') as f:\n",
    "        f.write(r.content)\n",
    "        time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps: Creating a CSV from Wine Spectator Data\n",
    "\n",
    "Here, we can use pandas (a Python library for data curation and analysis) to scrape data from the website and put it into a CSV file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#winespectator data to csv\n",
    "\n",
    "wine_page = requests.get('https://top100.winespectator.com/lists/')\n",
    "tree = html.fromstring(wine_page.content)\n",
    " \n",
    "# Get data from elements using XPath\n",
    "winery = tree.xpath(\"//span[@class = 'sort-text']/text()\") \n",
    "vintage = tree.xpath(\"//td[@class = 'vintage']/text()\")\n",
    "score = tree.xpath(\"//td[@class = 'score']/text()\")\n",
    "\n",
    "dataset = pd.DataFrame(list(zip(winery, vintage, score))) #combine our lists of data into a pandas dataframe\n",
    "dataset.to_csv('output.csv', sep=',', header=['Winery', 'Vintage', 'Score'], index=False)       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
